{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modèle CNN - FNN Multimodal\n",
        "\n",
        "\n",
        "### Importations"
      ],
      "metadata": {
        "id": "JBI4R3UI0T5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMOwLMJ8IhK4"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "#! python3 -m pip install requests\n",
        "#! python3 -m pip install json\n",
        "#! python3 -m pip install matplotib\n",
        "#! python3 -m pip install time\n",
        "#! python3 -m pip install numpy\n",
        "#! python3 -m pip install yfinance\n",
        "#! python3 -m pip install pandas\n",
        "#! python3 -m pip install torch\n",
        "#! python3 -m pip install sklearn\n",
        "#! python3 -m pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xms3QOYrIszS"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, ConfusionMatrixDisplay, confusion_matrix\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition du modèle"
      ],
      "metadata": {
        "id": "AYkgAw4X0z0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4XqpXIVIvj2"
      },
      "outputs": [],
      "source": [
        "class Torch_CNN(nn.Module):\n",
        "    def __init__(self, embed, num_pred):\n",
        "        super(Torch_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(embed, 32, kernel_size=3, padding=1) #Expansion graduelle\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.bn_pool1 = nn.BatchNorm2d(32) # meilleur performance a l'entrainement\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.bn_pool2 = nn.BatchNorm2d(64) # meilleur performance a l'entrainement\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.bn_pool3 = nn.BatchNorm2d(128) # meilleur performance a l'entrainement\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(128, num_pred) # Matrice dim (128 x nb de pred)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.leaky_relu(self.conv1(x)) #Leaky relu\n",
        "        x = self.pool1(x)\n",
        "        x = self.bn_pool1(x)\n",
        "\n",
        "        x = torch.nn.functional.leaky_relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.bn_pool2(x)\n",
        "\n",
        "        x = torch.nn.functional.leaky_relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.bn_pool3(x)\n",
        "\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Test FNN fusion\n",
        "class Torch_FFN(nn.Module):\n",
        "    def __init__(self, embed, hidden, num_pred):\n",
        "        super(Torch_FFN, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed)\n",
        "        self.fc1 = nn.Linear(embed, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, num_pred) # hidde x num de prediction\n",
        "        self.dropout = nn.Dropout(0.10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(torch.nn.functional.leaky_relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.nn.functional.leaky_relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Corrected Ensemble model\n",
        "class Torch_Ens(nn.Module):\n",
        "    def __init__(self, embed_img, ts_input_dim, hidden_dim, num_pred):\n",
        "        super().__init__()\n",
        "        self.cnn = Torch_CNN(embed_img, ts_input_dim) # sortie meme taille que input ts\n",
        "        self.fnn = Torch_FFN(ts_input_dim * 2, hidden_dim, hidden_dim) # 2 fois ts pour que le FNN est le meme nombre de feature par modalité\n",
        "        self.regression_head = nn.Linear(hidden_dim, num_pred)\n",
        "        self.classification_head = nn.Linear(hidden_dim, 2) # 2 options 0 ou 1\n",
        "\n",
        "    def forward(self, x_img, x_ts):\n",
        "        x_img_out = self.cnn(x_img)\n",
        "        x = torch.cat([x_img_out, x_ts], dim=1) # On fusionne [TS  + Features CNN] qui on la meme dim\n",
        "        x = self.fnn(x)\n",
        "        price_pred = self.regression_head(x)\n",
        "        direction_logits = self.classification_head(x)\n",
        "        return price_pred, direction_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp7oSCzeI2Ue"
      },
      "outputs": [],
      "source": [
        "#Test\n",
        "## El Model\n",
        "model = Torch_Ens(\n",
        "    embed_img=3, #Red / Green / Blue\n",
        "    ts_input_dim=10,\n",
        "    hidden_dim=256,\n",
        "    num_pred=1\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "x_img = torch.randn(batch_size, 3, 192, 192)\n",
        "x_ts = torch.randn(batch_size, 10)\n",
        "\n",
        "print(f\"Image shape: {x_img.shape}\")\n",
        "print(f\"Time series shape: {x_ts.shape}\")\n",
        "\n",
        "output = model(x_img, x_ts)\n",
        "print(f\"out shape: {output}\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    cnn_features = model.cnn(x_img)\n",
        "    print(f\"CNN feature std: {cnn_features.std()}\")\n",
        "    print(f\"CNN feature mean: {cnn_features.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation de données\n",
        "\n",
        "### Serie Futures"
      ],
      "metadata": {
        "id": "2rd2LD1R08bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6OGOCwdI5Gy"
      },
      "outputs": [],
      "source": [
        "start_date = \"2016-02-06\"\n",
        "end_date = \"2026-02-06\"\n",
        "Tickers = [\"ZC=F\", \"ZW=F\", \"ZM=F\", \"GF=F\", \"LE=F\"]\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for ticker_symbol in Tickers:\n",
        "    data = yf.download(ticker_symbol, start_date, progress=False, auto_adjust=False, interval=\"1d\")\n",
        "    adj_close_series = data['Adj Close'].squeeze().dropna()\n",
        "    ticker_df = adj_close_series.to_frame(name=ticker_symbol)\n",
        "    df_list.append(ticker_df)\n",
        "\n",
        "df_close = pd.concat(df_list, axis=1)\n",
        "df_close = df_close.dropna()\n",
        "\n",
        "column_names_map = {\n",
        "    \"ZW=F\": \"Wheat\",\n",
        "    \"ZM=F\": \"Soy\",\n",
        "    \"GF=F\": \"Cow_food\",\n",
        "    \"LE=F\": \"Cows\",\n",
        "    \"^IRX\": \"UST\",\n",
        "    \"NG=F\": \"Gaz\"\n",
        "}\n",
        "\n",
        "df_close.rename(columns=column_names_map, inplace=True)\n",
        "\n",
        "df_close.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Séries Météo"
      ],
      "metadata": {
        "id": "r05p15u51IHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2CjRDfRI70f"
      },
      "outputs": [],
      "source": [
        "\n",
        "latitude = 39.0997\n",
        "longitude = -94.5786 #Chicago\n",
        "\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "\n",
        "params = {\n",
        "    \"latitude\": latitude,\n",
        "    \"longitude\": longitude,\n",
        "    \"start_date\": start_date,\n",
        "    \"end_date\": end_date,\n",
        "    \"daily\": [\"temperature_2m_mean\",\"rain_sum\"],\n",
        "    \"timezone\": \"America/Chicago\",\n",
        "    \"models\": \"best_match\"\n",
        "}\n",
        "\n",
        "\n",
        "response = requests.get(url, params=params, timeout=30)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "\n",
        "    if 'daily' in data and 'time' in data['daily']:\n",
        "        # Créer le DataFrame\n",
        "        dates = pd.to_datetime(data['daily']['time'])\n",
        "        temperatures = data['daily']['temperature_2m_mean']\n",
        "        precipitation = data['daily']['rain_sum']\n",
        "\n",
        "        df_temp = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'temperature': temperatures,\n",
        "            'precipitation': precipitation\n",
        "        })\n",
        "\n",
        "df_temp = df_temp.set_index('date')\n",
        "df_close = df_close.join(df_temp, how='inner', rsuffix='_weather').dropna() #Intersection\n",
        "\n",
        "df_close.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats sur les séries temporelles"
      ],
      "metadata": {
        "id": "EBuMrn2w1BtM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GyO0GCoJEYt"
      },
      "outputs": [],
      "source": [
        "\n",
        "corr = df_close.corr()\n",
        "mean = df_close.mean()\n",
        "std = df_close.std()\n",
        "median = df_close.median()\n",
        "print(corr)\n",
        "\n",
        "print(f\"STD \\n {std}\")\n",
        "print(f\"MEAN \\n {mean}\")\n",
        "print(f\"MEDIAN \\n {median}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images"
      ],
      "metadata": {
        "id": "o7-uJca_1MzI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RIDHCncCJJLL"
      },
      "outputs": [],
      "source": [
        "\n",
        "params = {\n",
        "\"SERVICE\": \"WMS\",\n",
        "\"REQUEST\": \"GetMap\",\n",
        "\"VERSION\": \"1.3.0\",\n",
        "\"LAYERS\": \"MODIS_Terra_NDVI_8Day\", #Indices de vegetation\n",
        "\"CRS\": \"EPSG:4326\",\n",
        "\"BBOX\": \"40,-97,44,-87\",\n",
        "\"WIDTH\": 128, # On baisse la résolution\n",
        "\"HEIGHT\": 128,\n",
        "\"FORMAT\": \"image/png\",\n",
        "\"TIME\": \"01-01-2020\",\n",
        "\"TRANSPARENT\": \"FALSE\"\"\"\n",
        "}\n",
        "\n",
        "def Get_image_data(df):\n",
        "    dates =  df.index.strftime('%Y-%m-%d').tolist() #On change le format\n",
        "\n",
        "\n",
        "    images_dict={}\n",
        "    for i, date in enumerate(dates):\n",
        "        if i > 0:\n",
        "            time.sleep(0.05)  #On est gentil avec l'API de la Nasa\n",
        "        params['TIME'] = date\n",
        "        print(f\"{params['TIME']}\")\n",
        "        try:\n",
        "            API_endpoint = \"https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi\"\n",
        "            response = requests.get(API_endpoint, params=params, timeout=30)\n",
        "\n",
        "            if response.status_code == 200:  # Success\n",
        "                if response.content.startswith(b'\\x89PNG'):\n",
        "                    img = mpimg.imread(io.BytesIO(response.content))\n",
        "                    images_dict[date] = img\n",
        "                else:\n",
        "                    print(f\"Response not PNG for {date}\")\n",
        "                    print(response.text[:200])\n",
        "\n",
        "            else:\n",
        "                print(f\"Error {response.status_code} for {date}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur pour {date}: {str(e)}\")\n",
        "    return images_dict\n",
        "\n",
        "images_dict = Get_image_data(df_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Sx9zWZYlJK_U"
      },
      "outputs": [],
      "source": [
        "# Sauvegarde des séries\n",
        "np.savez_compressed('images.npz', **{date: img for date, img in images_dict.items()})\n",
        "df_close.to_csv('prices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement du modèle"
      ],
      "metadata": {
        "id": "DMoSXE681R9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4b6437d"
      },
      "outputs": [],
      "source": [
        "def compute_direction_labels(value, threshold=0.0):\n",
        "    labels = torch.ones_like(value, dtype=torch.long) #On met tt les labels en 1 puis on le passe en 0 si c'est une baisse\n",
        "    labels[value < -threshold] = 0\n",
        "\n",
        "    return labels\n",
        "\n",
        "def shift(xs, n):\n",
        "    e = np.empty_like(xs)\n",
        "    if n >= 0:\n",
        "        e[:n] = np.nan\n",
        "        e[n:] = xs[:-n]\n",
        "    else:\n",
        "        e[n:] = np.nan\n",
        "        e[:n] = xs[-n:]\n",
        "    return e #Pas de shift pour numpy dcp helper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "qdXPQ5OF1Xmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ca0010c"
      },
      "outputs": [],
      "source": [
        "## On crée un dataset\n",
        "images_npz = np.load('images.npz', mmap_mode = 'r')\n",
        "images_dict = {date: images_npz[date] for date in images_npz.files}\n",
        "df_close = pd.read_csv('prices.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "del images_npz  # LA MEMOIRE\n",
        "\n",
        "class full_dataset(Dataset):\n",
        "    def __init__(self, images_dict, df, target_col='ZC=F'):\n",
        "        self.images_dict = images_dict\n",
        "        self.df = df\n",
        "        self.target_col = target_col\n",
        "\n",
        "        self.valid_indices = []\n",
        "        dates = df.index.strftime('%Y-%m-%d').tolist()\n",
        "\n",
        "        for i in range(len(dates) - 1):\n",
        "            if dates[i] in images_dict:  # Check que on a une image a cette data\n",
        "                self.valid_indices.append(i)\n",
        "\n",
        "        self.ts_min = self.df.min(axis=0).values\n",
        "        self.ts_max = self.df.max(axis=0).values\n",
        "        self.target_min = self.df[self.target_col].values.min()\n",
        "        self.target_max = self.df[self.target_col].values.max()\n",
        "        self.image_min = min(img.min() for img in images_dict.values())\n",
        "        self.image_max = max(img.max() for img in images_dict.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices) - 20 #les dernieres valeurs na pas de target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        df_idx = self.valid_indices[idx]\n",
        "        date = self.df.index[df_idx].strftime('%Y-%m-%d')\n",
        "        next_date = self.df.index[df_idx + 20]\n",
        "\n",
        "\n",
        "        img = self.images_dict[date]\n",
        "\n",
        "        if img.ndim == 3 and img.shape[2] == 4: #Formatv torch si probleme\n",
        "            img = img[:, :, :3]\n",
        "\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        ts = self.df.iloc[df_idx].values\n",
        "        target = self.df.loc[next_date, self.target_col]\n",
        "        change = target - self.df.iloc[df_idx][self.target_col]\n",
        "\n",
        "        direction_labels = compute_direction_labels(torch.tensor(change, dtype=torch.float32), threshold=0)\n",
        "\n",
        "        # Norm & Tensor\n",
        "        ts_tensor = (ts - self.ts_min) / (self.ts_max - self.ts_min)\n",
        "        target = (target - self.target_min) / (self.target_max - self.target_min)\n",
        "        img = (img - self.image_min) / (self.image_max - self.image_min)\n",
        "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
        "        image_tensor = torch.from_numpy(img.copy()).float()\n",
        "        ts_tensor = torch.tensor(ts_tensor.copy()).float()\n",
        "\n",
        "        return image_tensor, ts_tensor, target_tensor, direction_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrainement"
      ],
      "metadata": {
        "id": "DC6V-QzC1e0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Training setup\n",
        "model = Torch_Ens(\n",
        "    embed_img=3,\n",
        "    ts_input_dim=7,\n",
        "    hidden_dim=256,\n",
        "    num_pred=1\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(lr=1e-3, weight_decay=1e-3, params=model.parameters())\n",
        "regression_criterion = nn.MSELoss()\n",
        "\n",
        "# Poid de la classification\n",
        "alpha = 0.05\n",
        "split_idx = int(0.8 * len(df_close.index))\n",
        "\n",
        "train_df = df_close.iloc[:split_idx, :]\n",
        "val_df = df_close.iloc[split_idx:, :]\n",
        "\n",
        "train_indices = df_close.iloc[:split_idx].index\n",
        "val_indices = df_close.iloc[split_idx:].index\n",
        "\n",
        "train_images = {k: v for k, v in images_dict.items() if k in train_indices} #On split bien les images aussi\n",
        "val_images = {k: v for k, v in images_dict.items() if k in val_indices}\n",
        "\n",
        "\n",
        "train_dataset = full_dataset(images_dict, train_df)\n",
        "val_dataset = full_dataset(images_dict, val_df)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "train_labels = []\n",
        "for _, _, _, direction_labels in train_loader:\n",
        "    train_labels.extend(direction_labels.numpy())\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "class_counts = np.bincount(train_labels)\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights = class_weights / class_weights.sum() * len(class_counts)  # calcul des poids des classe pour lutter contre les >\n",
        "weights = torch.FloatTensor(class_weights)\n",
        "\n",
        "classification_criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "## Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_reg_loss = 0\n",
        "    total_clf_loss = 0\n",
        "\n",
        "    for image, ts, target, direction_labels in train_loader:\n",
        "        # print(direction_labels)\n",
        "        image = image.float()\n",
        "        ts = ts.float()\n",
        "        target = target.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        price_pred, direction_logits = model(image, ts)\n",
        "\n",
        "        reg_loss = regression_criterion(price_pred, target)\n",
        "        clf_loss = classification_criterion(direction_logits, direction_labels)\n",
        "        loss = reg_loss + alpha * clf_loss\n",
        "\n",
        "        loss.backward() #Backward\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_reg_loss += reg_loss.item()\n",
        "        total_clf_loss += clf_loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_reg_loss = 0\n",
        "    val_clf_loss = 0\n",
        "\n",
        "    predictions_v = []\n",
        "    direction_preds_v = []\n",
        "    targets_v = []\n",
        "    direction_labels_v = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, ts, target, direction_labels in val_loader:\n",
        "            image = image.float()\n",
        "            ts = ts.float()\n",
        "            target = target.float().unsqueeze(1)\n",
        "\n",
        "            price_pred, direction_logits = model(image, ts)\n",
        "\n",
        "            reg_loss = regression_criterion(price_pred, target)\n",
        "            clf_loss = classification_criterion(direction_logits, direction_labels)\n",
        "\n",
        "            val_reg_loss += reg_loss.item()\n",
        "            val_clf_loss += clf_loss.item()\n",
        "            val_loss += (reg_loss + alpha * clf_loss).item()\n",
        "\n",
        "            predictions_v.append(price_pred)\n",
        "            direction_preds_v.append(torch.argmax(direction_logits, dim=1))\n",
        "            targets_v.append(target)\n",
        "            direction_labels_v.append(direction_labels)\n",
        "\n",
        "    predictions_v = torch.cat(predictions_v, dim=0).squeeze().numpy()\n",
        "    direction_preds_v = torch.cat(direction_preds_v, dim=0).numpy()\n",
        "    targets_v = torch.cat(targets_v, dim=0).squeeze().numpy()\n",
        "    direction_labels_v = torch.cat(direction_labels_v, dim=0).numpy() #On passe tt en numpy\n",
        "\n",
        "    direction_accuracy = (direction_preds_v == direction_labels_v).mean() * 100\n",
        "\n",
        "    pred_pct = (predictions_v - shift(predictions_v, 1)) / (shift(predictions_v, 1) + 1e-8)\n",
        "    target_pct = (targets_v - shift(targets_v, 1)) / (shift(targets_v, 1) + 1e-8)\n",
        "\n",
        "    pred_pct = pred_pct[1:]\n",
        "    target_pct = target_pct[1:]\n",
        "\n",
        "    winrate_regression = ((pred_pct > 0) == (target_pct > 0)).mean() * 100\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(f\"  Train - Total: {total_loss/len(train_loader):.4f}, Reg: {total_reg_loss/len(train_loader):.4f}, Clf: {total_clf_loss/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Total: {val_loss/len(val_loader):.4f}, Reg: {val_reg_loss/len(val_loader):.4f}, Clf: {val_clf_loss/len(val_loader):.4f}\")\n",
        "    print(f\"R-squared: {metrics.r2_score(targets_v, predictions_v):.4f}\")\n",
        "    print(f\"  Direction Class Acc: {direction_accuracy:.2f}%\")\n",
        "    print(f\"  Winrate (Regression): {winrate_regression:.2f}%\")"
      ],
      "metadata": {
        "id": "x-OHlY9SFFQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comapraison et résulats"
      ],
      "metadata": {
        "id": "kuXKzfnD1jay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e27a4f5f"
      },
      "outputs": [],
      "source": [
        "#Modèle de regresison multiple classique\n",
        "\n",
        "X_train_lr = []\n",
        "y_train_lr = []\n",
        "for _, ts, target, _ in train_loader:\n",
        "    X_train_lr.append(ts.flatten(1))\n",
        "    y_train_lr.append(target.unsqueeze(1))\n",
        "\n",
        "X_train_lr = torch.cat(X_train_lr, dim=0).numpy()\n",
        "y_train_lr = torch.cat(y_train_lr, dim=0).numpy().squeeze()\n",
        "\n",
        "X_val_lr = []\n",
        "y_val_lr = []\n",
        "for _, ts, target, _ in val_loader:\n",
        "    X_val_lr.append(ts.flatten(1))\n",
        "    y_val_lr.append(target.unsqueeze(1))\n",
        "\n",
        "X_val_lr = torch.cat(X_val_lr, dim=0).numpy()\n",
        "y_val_lr = torch.cat(y_val_lr, dim=0).numpy().squeeze()\n",
        "\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "\n",
        "predictions_lr = linear_model.predict(X_val_lr)\n",
        "\n",
        "r2_lr = r2_score(y_val_lr, predictions_lr)\n",
        "mse_lr = mean_squared_error(y_val_lr, predictions_lr)\n",
        "\n",
        "print(f\"R-squared (Linear Regression): {r2_lr:.4f}\")\n",
        "print(f\"MSE (Linear Regression): {mse_lr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3256112"
      },
      "outputs": [],
      "source": [
        "#On sort les pred\n",
        "\n",
        "all_predictions = predictions_v\n",
        "all_targets = targets_v\n",
        "actual_directions = direction_labels_v\n",
        "all_directions = direction_preds_v\n",
        "\n",
        "#On repasse en prix\n",
        "denorm_all_predictions = all_predictions * (val_dataset.target_max - val_dataset.target_min) + val_dataset.target_min\n",
        "denorm_all_targets = all_targets * (val_dataset.target_max - val_dataset.target_min) + val_dataset.target_min\n",
        "\n",
        "denorm_predictions_lr = predictions_lr * int(val_dataset.target_max - val_dataset.target_min) + val_dataset.target_min\n",
        "denorm_y_val_lr = y_val_lr * int(val_dataset.target_max - val_dataset.target_min) + val_dataset.target_min\n",
        "\n",
        "#Les errors\n",
        "denorm_all_errors = denorm_all_predictions - denorm_all_targets\n",
        "\n",
        "# --- Plotting --- Merci Claude\n",
        "\n",
        "# Plot 1: Predictions vs Actuals (Denormalized) - Torch_Ens and Linear Regression\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(denorm_all_targets, c='crimson', label='Réel', linewidth=2)\n",
        "plt.plot(denorm_all_predictions, c='blue', label='Prédit (Torch_Ens)', linewidth=2, alpha=0.7)\n",
        "plt.plot(denorm_predictions_lr, c='orange', label='Prédit (Linear Regression)', linewidth=2, alpha=0.7)\n",
        "plt.xlabel('Temps', fontsize=12)\n",
        "plt.ylabel('Prix (Dénormalisé)', fontsize=12)\n",
        "plt.title(\"Prédictions vs Réelles (Dénormalisé)\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Error distribution (Denormalized) - Torch_Ens\n",
        "plt.figure(figsize=(10, 6))\n",
        "mse_denorm_ens = np.mean(denorm_all_errors**2)\n",
        "plt.hist(denorm_all_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.title(f\"Distribution des Erreurs (MSE: {mse_denorm_ens:.4f}) - Torch_Ens\", fontsize=14)\n",
        "plt.xlabel('Erreur (Dénormalisé)', fontsize=12)\n",
        "plt.ylabel('Fréquence', fontsize=12)\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Direction predictions (using Denormalized Actuals) - Torch_Ens\n",
        "plt.figure(figsize=(10, 6))\n",
        "direction_map = {0: 'Baisse', 1: 'Hausse'}\n",
        "direction_colors = {0: 'red',1: 'green'}\n",
        "\n",
        "# Plot the actual prices\n",
        "plt.plot(denorm_all_targets, c='black', linewidth=1, alpha=0.3, label='Prix réel')\n",
        "\n",
        "# Plot scatter points for each direction separately to get correct legend entries\n",
        "for direction_class in sorted(direction_map.keys()):\n",
        "    mask = (all_directions == direction_class)\n",
        "    if mask.sum() > 0:\n",
        "        plt.scatter(np.arange(len(denorm_all_targets))[mask],\n",
        "                           denorm_all_targets[mask],\n",
        "                           c=direction_colors[direction_class],\n",
        "                           alpha=0.6, s=30,\n",
        "                           label=f'Prédit: {direction_map[direction_class]}')\n",
        "\n",
        "plt.xlabel('Temps', fontsize=12)\n",
        "plt.ylabel('Prix (Dénormalisé)', fontsize=12)\n",
        "plt.title(\"Prédictions de Direction (Dénormalisé) - FNN et TSs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Confusion matrix for direction - Torch_Ens (no change needed as it uses labels)\n",
        "plt.figure(figsize=(8, 8))\n",
        "cm = confusion_matrix(actual_directions, all_directions, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                               display_labels=['Baisse', 'Hausse'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Matrice de Confusion (Direction) - FNN et TS\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Additional metrics (Denormalized) ---\n",
        "print(\"\\n=== Métriques Finales (Dénormalisées) ===\")\n",
        "\n",
        "# Metrics for Torch_Ens\n",
        "print(\"\\n--- Torch_Ens Model ---\")\n",
        "mse_ens_denorm = np.mean(denorm_all_errors**2)\n",
        "mae_ens_denorm = np.mean(np.abs(denorm_all_errors))\n",
        "r2_ens_denorm = metrics.r2_score(denorm_all_targets, denorm_all_predictions)\n",
        "print(f\"MSE: {mse_ens_denorm:.4f}\")\n",
        "print(f\"MAE: {mae_ens_denorm:.4f}\")\n",
        "print(f\"R-squared: {r2_ens_denorm:.4f}\")\n",
        "\n",
        "# Winrate for Regression Head (Torch_Ens)\n",
        "pred_returns_ens_denorm = (denorm_all_predictions[1:] - denorm_all_predictions[:-1]) / (denorm_all_predictions[:-1] + 1e-8)\n",
        "actual_returns_ens_denorm = (denorm_all_targets[1:] - denorm_all_targets[:-1]) / (denorm_all_targets[:-1] + 1e-8)\n",
        "winrate_regression_ens_denorm = ((pred_returns_ens_denorm > 0) == (actual_returns_ens_denorm > 0)).mean() * 100\n",
        "print(f\"Winrate (Regression Head): {winrate_regression_ens_denorm:.2f}%\")\n",
        "\n",
        "# Direction Classification Accuracy (Torch_Ens) - no change as it's label-based\n",
        "direction_acc = (actual_directions == all_directions).mean() * 100\n",
        "print(f\"Direction Classification Accuracy: {direction_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# Metrics for Scikit-learn Linear Regression\n",
        "print(\"\\n--- Scikit-learn Linear Regression Model ---\")\n",
        "mse_lr_denorm = metrics.mean_squared_error(denorm_y_val_lr, denorm_predictions_lr)\n",
        "mae_lr_denorm = np.mean(np.abs(denorm_y_val_lr - denorm_predictions_lr))\n",
        "r2_lr_denorm = metrics.r2_score(denorm_y_val_lr, denorm_predictions_lr)\n",
        "print(f\"MSE: {mse_lr_denorm:.4f}\")\n",
        "print(f\"MAE: {mae_lr_denorm:.4f}\")\n",
        "print(f\"R-squared: {r2_lr_denorm:.4f}\")\n",
        "\n",
        "# Winrate for Scikit-learn Linear Regression\n",
        "pred_returns_lr_denorm = (denorm_predictions_lr[1:] - denorm_predictions_lr[:-1]) / (denorm_predictions_lr[:-1] + 1e-8)\n",
        "actual_returns_lr_denorm = (denorm_y_val_lr[1:] - denorm_y_val_lr[:-1]) / (denorm_y_val_lr[:-1] + 1e-8)\n",
        "winrate_regression_lr_denorm = ((pred_returns_lr_denorm > 0) == (actual_returns_lr_denorm > 0)).mean() * 100\n",
        "print(f\"Winrate (Regression): {winrate_regression_lr_denorm:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backtest"
      ],
      "metadata": {
        "id": "lKiDxlhV1nzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### backtest sur validation\n",
        "\n",
        "# Create a DataFrame from the validation results\n",
        "df_backtest = pd.DataFrame({\n",
        "    \"Pt\": denorm_all_targets,\n",
        "    \"prediction\": denorm_all_predictions,\n",
        "    \"classification\": direction_preds_v\n",
        "})\n",
        "\n",
        "\n",
        "# Prix futur (shift by 20 days as per dataset logic)\n",
        "df_backtest[\"Future_Price\"] = df_backtest[\"Pt\"].shift(-20)\n",
        "\n",
        "df_backtest = df_backtest.dropna()\n",
        "\n",
        "# Define transaction cost (e.g., 0.1% per trade)\n",
        "transaction_cost_per_trade = 0.01\n",
        "\n",
        "# CONDITIONS\n",
        "long_condition = (\n",
        "    (df_backtest[\"classification\"] == 1) &\n",
        "    (df_backtest[\"prediction\"] > df_backtest[\"Pt\"])\n",
        ")\n",
        "short_condition = (\n",
        "    (df_backtest[\"classification\"] == 0) &\n",
        "    (df_backtest[\"prediction\"] < df_backtest[\"Pt\"])\n",
        ")\n",
        "\n",
        "# Rendements\n",
        "# Long trades (buy when conditions met)\n",
        "df_backtest[\"Long_return\"] = np.where(\n",
        "    long_condition,\n",
        "    np.log(df_backtest[\"Future_Price\"] / df_backtest[\"Pt\"]) - transaction_cost_per_trade,\n",
        "    0\n",
        ")\n",
        "\n",
        "# Short trades (sell when conditions met)\n",
        "df_backtest[\"Short_return\"] = np.where(\n",
        "    short_condition,\n",
        "    np.log(df_backtest[\"Pt\"] / df_backtest[\"Future_Price\"]) - transaction_cost_per_trade,\n",
        "    0\n",
        ")\n",
        "\n",
        "# Long Only Strategy\n",
        "df_backtest[\"Strategy_Long_Only\"] = df_backtest[\"Long_return\"]\n",
        "\n",
        "# Long / Short Strategy\n",
        "df_backtest[\"Strategy_Long_Short\"] = df_backtest[\"Long_return\"] + df_backtest[\"Short_return\"]\n",
        "\n",
        "# Buy and Hold (Holding the Future) Strategy\n",
        "df_backtest[\"Buy_Hold_Return\"] = np.log(df_backtest[\"Pt\"] / df_backtest[\"Pt\"].shift(1))\n",
        "df_backtest[\"Buy_Hold_Return\"] = df_backtest[\"Buy_Hold_Return\"].fillna(0)\n",
        "\n",
        "# Calculate Equity Curves\n",
        "df_backtest[\"Equity_Long_Only\"] = np.exp(df_backtest[\"Strategy_Long_Only\"].cumsum())\n",
        "df_backtest[\"Equity_Long_Short\"] = np.exp(df_backtest[\"Strategy_Long_Short\"].cumsum())\n",
        "df_backtest[\"Equity_Buy_Hold\"] = np.exp(df_backtest[\"Buy_Hold_Return\"].cumsum())\n",
        "\n",
        "\n",
        "# Print Summary Statistics\n",
        "print(\"\\n--- Backtesting Performance Metrics ---\")\n",
        "\n",
        "trading_days_per_year = 252\n",
        "\n",
        "def calculate_metrics(returns_series, strategy_name):\n",
        "    total_return = returns_series.sum()\n",
        "    num_trades = (returns_series != 0).sum()\n",
        "    avg_return_per_trade = returns_series.sum() / num_trades if num_trades > 0 else 0\n",
        "    num_trading_periods = len(returns_series[returns_series != 0])\n",
        "\n",
        "    if strategy_name == \"Buy & Hold\":\n",
        "        daily_avg_log_return = returns_series.mean()\n",
        "        annualized_return = (sum(returns_series)+1)**(252/456) - 1\n",
        "    else:\n",
        "        daily_avg_log_return = returns_series.mean() # Average over all periods\n",
        "        annualized_return =  (sum(returns_series)+1)**(252/456) - 1 # Assuming daily average return\n",
        "\n",
        "    print(f\"\\nStrategy: {strategy_name}\")\n",
        "    print(f\"  Total Log Return: {total_return:.4f}\")\n",
        "    print(f\"  Number of Trading Days: {len(returns_series)}\")\n",
        "    print(f\"  Annualized Return: {annualized_return * 100:.2f}%\")\n",
        "\n",
        "calculate_metrics(df_backtest[\"Strategy_Long_Only\"], \"Long Only\")\n",
        "calculate_metrics(df_backtest[\"Strategy_Long_Short\"], \"Long/Short\")\n",
        "calculate_metrics(df_backtest[\"Buy_Hold_Return\"], \"Buy & Hold\")\n",
        "\n",
        "\n",
        "# Graphique\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_backtest[\"Equity_Long_Only\"], label=\"Long Only\")\n",
        "plt.plot(df_backtest[\"Equity_Long_Short\"], label=\"Long / Short\")\n",
        "plt.plot(df_backtest[\"Equity_Buy_Hold\"], label=\"Buy & Hold\")\n",
        "\n",
        "plt.title(\"Backtest sur les données de validation\")\n",
        "plt.xlabel(\"Temps()\")\n",
        "plt.ylabel(\"Valeur en base 1\")\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kj4-O_lhzQtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Backtest Options\n",
        "\n",
        "df_backtest['Pt_sd'] = df_backtest['Pt'].rolling(window=30, min_periods = 1).std()\n",
        "df_backtest['vol'] = df_backtest['Pt_sd']/df_backtest['Pt'] * np.sqrt(365)\n",
        "rf = 1.03**(20/360)-1\n",
        "T = 20/360\n",
        "\n",
        "transaction_cost_per_trade = 0.01 # 1% transaction cost\n",
        "\n",
        "d1 = (np.log(df_backtest['Pt'] / df_backtest['Pt']) + (rf + 0.5 * df_backtest['vol']**2) * T) / (df_backtest['vol'] * np.sqrt(T))\n",
        "d2 = d1 - df_backtest['vol'] * np.sqrt(T)\n",
        "\n",
        "df_backtest['Call_Price'] = (\n",
        "    df_backtest['Pt'] * norm.cdf(d1) -\n",
        "    df_backtest['Pt'] * np.exp(-rf * T) * norm.cdf(d2)\n",
        ")\n",
        "\n",
        "df_backtest['Put_ATM'] = (\n",
        "    df_backtest['Pt'] * np.exp(-rf * T) * norm.cdf(-d2) -\n",
        "    df_backtest['Pt'] * norm.cdf(-d1)\n",
        ")\n",
        "# On met en place B&S\n",
        "# Retour au Long Call Only ou au Long Call / Long Put\n",
        "\n",
        "# Potentiel de rendement plus fort que breakeven\n",
        "cond_long_call = (\n",
        "    (df_backtest['classification'] == 1) &\n",
        "    (df_backtest['prediction'] > (df_backtest['Pt'] + df_backtest['Call_Price']))\n",
        ")\n",
        "\n",
        "cond_long_put = (\n",
        "    (df_backtest['classification'] == 0) &\n",
        "    (df_backtest['prediction'] < (df_backtest['Pt'] - df_backtest['Put_ATM']))\n",
        ")\n",
        "\n",
        "intrinsic_value_call = np.maximum(df_backtest['Future_Price'] - df_backtest['Pt'], 0)\n",
        "df_backtest['PnL_Call'] = np.where(\n",
        "    cond_long_call,\n",
        "    (intrinsic_value_call - df_backtest['Call_Price']) * (1-transaction_cost_per_trade),\n",
        "    0\n",
        ")\n",
        "\n",
        "intrinsic_value_put = np.maximum(df_backtest['Pt'] - df_backtest['Future_Price'], 0)\n",
        "df_backtest['PnL_Put'] = np.where(\n",
        "    cond_long_put,\n",
        "    (intrinsic_value_put - df_backtest['Put_ATM']) * (1-transaction_cost_per_trade),\n",
        "    0\n",
        ")\n",
        "\n",
        "df_backtest['Call_Only'] = df_backtest['PnL_Call'].cumsum()\n",
        "df_backtest['Call_Put'] = df_backtest['PnL_Call'].cumsum() + df_backtest['PnL_Put'].cumsum()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_backtest['Call_Only'], label='Stratégie Call Only', color='blue')\n",
        "plt.plot(df_backtest['Call_Put'], label='Stratégie Call + Put', color='green')\n",
        "\n",
        "plt.title('Performance Cumulée (PnL en $ Absolu)')\n",
        "plt.xlabel('Jours')\n",
        "plt.ylabel('Profit Cumulé ($)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# Short d'options\n",
        "\n",
        "cond_short_put = (df_backtest['classification'] == 1) & (df_backtest['prediction'] > (df_backtest['Pt'] + df_backtest['Put_ATM']))\n",
        "cond_short_call = (df_backtest['classification'] == 0) & (df_backtest['prediction'] < (df_backtest['Pt'] - df_backtest['Call_Price']))\n",
        "\n",
        "intrinsic_value_call = np.maximum(df_backtest['Future_Price'] - df_backtest['Pt'], 0)\n",
        "df_backtest['PnL_SCall'] = np.where(\n",
        "    cond_short_call,\n",
        "    df_backtest['Call_Price'] - intrinsic_value_call - transaction_cost_per_trade,\n",
        "    0\n",
        ")\n",
        "\n",
        "intrinsic_value_put = np.maximum(df_backtest['Pt'] - df_backtest['Future_Price'], 0)\n",
        "df_backtest['PnL_SPut'] = np.where(\n",
        "    cond_short_put,\n",
        "    df_backtest['Put_ATM'] - intrinsic_value_put - transaction_cost_per_trade,\n",
        "    0\n",
        ")\n",
        "\n",
        "df_backtest['SCall_SPut'] = df_backtest['PnL_SCall'].cumsum() + df_backtest['PnL_SPut'].cumsum()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_backtest['SCall_SPut'], label='Stratégie Short Call + Put', color='red')\n",
        "\n",
        "plt.title('Performance Cumulée (PnL en $ Absolu)')\n",
        "plt.xlabel('Jours')\n",
        "plt.ylabel('Profit Cumulé ($)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "viypw2uNOtAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nouveau graphique: Points d'accord des deux têtes du modèle pour le trading\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot du prix réel\n",
        "plt.plot(df_backtest.index, df_backtest['Pt'], c='black', linewidth=1, alpha=0.5, label='Prix Réel')\n",
        "\n",
        "# Calculate actual future direction based on Future_Price\n",
        "df_backtest['Actual_Future_Direction'] = (df_backtest['Future_Price'] > df_backtest['Pt']).astype(int)\n",
        "\n",
        "# Conditions de trading (basées sur l'accord des deux têtes)\n",
        "long_signals = (df_backtest['classification'] == 1) & (df_backtest['prediction'] > df_backtest['Pt'])\n",
        "short_signals = (df_backtest['classification'] == 0) & (df_backtest['prediction'] < df_backtest['Pt'])\n",
        "\n",
        "# Distinguer les signaux corrects et incorrects\n",
        "correct_long_signals = long_signals & (df_backtest['Actual_Future_Direction'] == 1)\n",
        "incorrect_long_signals = long_signals & (df_backtest['Actual_Future_Direction'] == 0)\n",
        "\n",
        "correct_short_signals = short_signals & (df_backtest['Actual_Future_Direction'] == 0)\n",
        "incorrect_short_signals = short_signals & (df_backtest['Actual_Future_Direction'] == 1)\n",
        "\n",
        "# Plot des signaux d'achat (Long) - Corrects\n",
        "plt.scatter(df_backtest.index[correct_long_signals], df_backtest['Pt'][correct_long_signals],\n",
        "            color='green', marker='^', s=100, label='Signal Achat Correct (Prix monte)', alpha=0.9)\n",
        "\n",
        "# Plot des signaux de vente (Short) - Corrects\n",
        "plt.scatter(df_backtest.index[correct_short_signals], df_backtest['Pt'][correct_short_signals],\n",
        "            color='red', marker='v', s=100, label='Signal Vente Correct (Prix baisse)', alpha=0.9)\n",
        "\n",
        "# Plot des signaux d'achat (Long) - Incorrects\n",
        "plt.scatter(df_backtest.index[incorrect_long_signals], df_backtest['Pt'][incorrect_long_signals],\n",
        "            color='orange', marker='x', s=100, label='Signal Achat Incorrect (Prix baisse)', alpha=0.9)\n",
        "\n",
        "# Plot des signaux de vente (Short) - Incorrects\n",
        "plt.scatter(df_backtest.index[incorrect_short_signals], df_backtest['Pt'][incorrect_short_signals],\n",
        "            color='purple', marker='o', s=100, label='Signal Vente Incorrect (Prix monte)', alpha=0.9)\n",
        "\n",
        "\n",
        "plt.xlabel('Temps', fontsize=12)\n",
        "plt.ylabel('Prix (Dénormalisé)', fontsize=12)\n",
        "plt.title('Signaux de Trading (Corrects vs Incorrects) basés sur l\\'Accord des Deux Têtes du Modèle', fontsize=14)\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cim2j445yYs-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}